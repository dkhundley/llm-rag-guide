{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"source":["# RAG Advanced Ingestion\n","In this notebook, we will cover how to apply advanced techniques to your retrieval augmented generation (RAG) ingestion pipeline!"]},{"cell_type":"markdown","metadata":{},"source":["# Notebook Setup\n","Let's set ourselves up for success by performing the necessary pip installs and importing the necessary libraries."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-23T20:57:18.918905Z","iopub.status.busy":"2024-08-23T20:57:18.918515Z","iopub.status.idle":"2024-08-23T20:57:18.924010Z","shell.execute_reply":"2024-08-23T20:57:18.922901Z","shell.execute_reply.started":"2024-08-23T20:57:18.918874Z"},"trusted":true},"outputs":[],"source":["# Performing the necessary pip installs\n","import os\n","if 'KAGGLE_URL_BASE' in os.environ:\n","    from pip_install import perform_pip_install\n","    # perform_pip_install()"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T21:40:25.170270Z","iopub.status.busy":"2024-08-30T21:40:25.169334Z","iopub.status.idle":"2024-08-30T21:40:25.176612Z","shell.execute_reply":"2024-08-30T21:40:25.175331Z","shell.execute_reply.started":"2024-08-30T21:40:25.170209Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/vscode/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","/home/vscode/.local/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAIBase has conflict with protected namespace \"model_\".\n","\n","You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n","  warnings.warn(\n","/home/vscode/.local/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAICommon has conflict with protected namespace \"model_\".\n","\n","You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n","  warnings.warn(\n","/home/vscode/.local/lib/python3.11/site-packages/ragas/metrics/__init__.py:4: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n","\n","For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n","with: `from pydantic import BaseModel`\n","or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n","\n","  from ragas.metrics._answer_correctness import AnswerCorrectness, answer_correctness\n","/home/vscode/.local/lib/python3.11/site-packages/ragas/metrics/__init__.py:7: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n","\n","For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n","with: `from pydantic import BaseModel`\n","or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n","\n","  from ragas.metrics._context_entities_recall import (\n"]}],"source":["# Importing the necessary Python libraries\n","import json\n","import time\n","import yaml\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from datasets import Dataset\n","from langchain.vectorstores import FAISS\n","from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n","from langchain_community.document_loaders import DataFrameLoader\n","from langchain_experimental.text_splitter import SemanticChunker\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from ragas import evaluate\n","from ragas.metrics import (\n","    faithfulness,\n","    answer_relevancy,\n","    answer_correctness,\n","    context_recall,\n","    context_precision\n",")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T21:40:39.864602Z","iopub.status.busy":"2024-08-30T21:40:39.864176Z","iopub.status.idle":"2024-08-30T21:40:40.506425Z","shell.execute_reply":"2024-08-30T21:40:40.505429Z","shell.execute_reply.started":"2024-08-30T21:40:39.864570Z"},"trusted":true},"outputs":[],"source":["# Loading the API keys from Kaggle Secrets\n","if 'KAGGLE_URL_BASE' in os.environ:\n","    from load_api_keys import load_api_keys\n","    api_keys = load_api_keys()\n","    \n","# Loading the API keys from local file\n","else:\n","    with open('../keys/api_keys.yaml', 'r') as file:\n","        api_keys = yaml.safe_load(file)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-30T21:41:30.326554Z","iopub.status.busy":"2024-08-30T21:41:30.326085Z","iopub.status.idle":"2024-08-30T21:41:30.371463Z","shell.execute_reply":"2024-08-30T21:41:30.370324Z","shell.execute_reply.started":"2024-08-30T21:41:30.326518Z"},"trusted":true},"outputs":[],"source":["# Loading in the sample datasets from file\n","if 'KAGGLE_URL_BASE' in os.environ:\n","    df_kis = pd.read_csv('/kaggle/input/synthetic-it-related-knowledge-items/synthetic_knowledge_items.csv')\n","    df_validation = pd.read_csv('/kaggle/input/sample-rag-knowledge-item-dataset/rag_sample_qas_from_kis.csv')\n","else:\n","    df_kis = pd.read_csv('../data/synthetic_knowledge_items.csv')\n","    df_validation = pd.read_csv('../data/rag_sample_qas_from_kis.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Dropping alt_ki_text from the df_kis DataFrame\n","df_kis.drop(columns = ['alt_ki_text'], inplace = True)\n","\n","# Viewing the first few rows of the knowledge item dataframe\n","df_kis.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Dropping any unnecessary columns from the validation DataFrame\n","df_validation.drop(columns = ['ki_topic', 'ki_text'], inplace = True)\n","\n","# Renaming the remaining columns\n","df_validation.rename(columns = {\n","    'sample_question': 'question',\n","    'sample_ground_truth': 'ground_truth'\n","}, inplace = True)\n","\n","# Viewing the first few rows of the validation dataframe\n","df_validation.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Chunking Strategies\n","In this section, we will be exploring the impact of different chunking strategies on the performance of our RAG system. We will be using the `Ragas` framework to evaluate the performance of each chunking strategy. \n","\n","To do this, we will be using the `df_validation` DataFrame that we created in the previous section. This DataFrame contains a set of questions along with the ground truth answers that we will be using to evaluate the performance of our chunking strategies.\n","\n","For each chunking strategy, we will be creating a FAISS index and then using the `df_validation` DataFrame to evaluate the performance of the chunking strategy. We will be using the `Ragas` framework to evaluate the performance of each chunking strategy.\n","\n","We will be using the `answer_correctness`, `answer_relevancy`, `faithfulness`, `context_recall`, and `context_precision` metrics to evaluate the performance of each chunking strategy."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Creating the ground truth simulation prompt template\n","ANSWER_GENERATION_PROMPT = '''You are an expert evaluator for question-answering systems. Your task is to provide the ideal answer based on the given question and context. Please follow these guidelines:\n","\n","1. Question: {question}\n","\n","2. Context: {context}\n","\n","3. Instructions:\n","   - Carefully analyze the question and the provided context.\n","   - Formulate a comprehensive and accurate answer based solely on the information given in the context.\n","   - Ensure your answer directly addresses the question.\n","   - Include all relevant information from the context, but do not add any external knowledge.\n","   - If the context doesn't contain enough information to fully answer the question, state this clearly and provide the best possible partial answer.\n","   - Use a formal, objective tone.\n","\n","Remember, your goal is to provide the ideal answer that should be used as the benchmark for evaluating the AI's performance.'''"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-23T21:06:28.349885Z","iopub.status.busy":"2024-08-23T21:06:28.349518Z","iopub.status.idle":"2024-08-23T21:06:28.382093Z","shell.execute_reply":"2024-08-23T21:06:28.380907Z","shell.execute_reply.started":"2024-08-23T21:06:28.349857Z"},"trusted":true},"outputs":[],"source":["# Setting the OpenAI API key as an environment variable\n","os.environ['OPENAI_API_KEY'] = api_keys['OPENAI_API_KEY']\n","\n","# Setting up the embedding algorithm\n","embedding_algorithm = OpenAIEmbeddings()\n","\n","# Setting up the chat model\n","chat_model = ChatOpenAI(model = 'chatgpt-4o-latest')\n","\n","# Creating the prompt engineering emplate to generate the simulated ground truth\n","answer_generation_prompt = ChatPromptTemplate.from_messages(messages = [\n","    HumanMessagePromptTemplate.from_template(template = ANSWER_GENERATION_PROMPT)\n","])\n","\n","# Creating the inference chain to generate the simulated answer\n","answer_generation_chain = answer_generation_prompt | chat_model"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def pandas_to_ragas(df):\n","    '''\n","    Converts a Pandas DataFrame into a Ragas-compatible dataset\n","    \n","    Inputs:\n","        - df (Pandas DataFrame): The input DataFrame to be converted\n","        \n","    Returns:\n","        - ragas_testset (Hugging Face Dataset): A Hugging Face dataset compatible with the Ragas framework\n","    '''\n","    # Ensure all text columns are strings and handle NaN values\n","    text_columns = ['question', 'ground_truth', 'answer']\n","    for col in text_columns:\n","        df[col] = df[col].fillna('').astype(str)\n","        \n","    # Convert 'contexts' to a list of lists\n","    df['contexts'] = df['contexts'].fillna('').astype(str).apply(lambda x: [x] if x else [])\n","    \n","    # Converting the DataFrame to a dictionary\n","    data_dict = df[['question', 'contexts', 'answer', 'ground_truth']].to_dict('list')\n","    \n","    # Loading the dictionary as a Hugging Face dataset\n","    ragas_testset = Dataset.from_dict(data_dict)\n","    \n","    return ragas_testset"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Defining different chunk sizes to experiment with\n","chunk_sizes = [100, 200, 500, 1000, 'semantic']\n","\n","# Initializing dictionary to store FAISS indexes and DataFrame for evaluation results\n","faiss_indexes = {}\n","\n","# Checking if a CSV with experimentation results exists\n","if os.path.exists('../data/chunking_experiment_results.csv'):\n","    df_chunking_results = pd.read_csv('../data/chunking_experiment_results.csv')\n","else:\n","    df_chunking_results = pd.DataFrame(columns = ['chunk_size', 'answer_correctness', 'answer_relevancy', 'faithfulness', 'context_recall', 'context_precision'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading the documents\n","documents = DataFrameLoader(df_kis, page_content_column = 'ki_text').load()\n","\n","# Iterating through each chunk size\n","for chunk_size in chunk_sizes:\n","\n","    # Checking if the chunk size has already been evaluated\n","    if str(chunk_size) not in df_chunking_results['chunk_size'].values:\n","\n","        # Checking what kind of chunking we are doing here\n","        if chunk_size == 'semantic':\n","    \n","            # Creating a semantic text splitter\n","            text_splitter = SemanticChunker(embeddings = embedding_algorithm)\n","\n","        else:\n","\n","            # Creating a recursive text splitter with the current chunk size\n","            text_splitter = RecursiveCharacterTextSplitter(\n","                chunk_size = chunk_size,\n","                chunk_overlap = 20,\n","                length_function = len\n","            )\n","\n","        \n","        # Splitting the documents into chunks\n","        chunks = text_splitter.split_documents(documents)\n","        \n","        # Creating FAISS index for the current chunk size\n","        faiss_index = FAISS.from_documents(chunks, embedding_algorithm)\n","        \n","        # Storing the index\n","        faiss_indexes[chunk_size] = faiss_index\n","\n","        poxdpdadsf\n","        \n","        # Generating answers using the retriever and answer generation chain\n","        df_validation['contexts'] = df_validation['question'].apply(lambda q: retriever.invoke(q)[0].page_content)\n","        df_validation['answer'] = df_validation.apply(\n","            lambda row: answer_generation_chain.invoke({\n","                'question': row['question'],\n","                'context': row['contexts']\n","            }).content,\n","            axis = 1\n","        )\n","        \n","        # Converting the DataFrame to a Ragas-compatible dataset\n","        ragas_testset = pandas_to_ragas(df_validation)\n","        \n","        # Evaluating the chunking strategy using ragas\n","        result = evaluate(\n","            dataset = ragas_testset,\n","            llm = chat_model,\n","            metrics = [\n","                answer_correctness,\n","                answer_relevancy,\n","                faithfulness,\n","                context_recall,\n","                context_precision\n","            ]\n","        )\n","        \n","        # Storing the evaluation results in the DataFrame\n","        new_row = pd.DataFrame({\n","            'chunk_size': [chunk_size],\n","            'answer_correctness': [result['answer_correctness']],\n","            'answer_relevancy': [result['answer_relevancy']],\n","            'faithfulness': [result['faithfulness']],\n","            'context_recall': [result['context_recall']],\n","            'context_precision': [result['context_precision']]\n","        })\n","\n","        df_chunking_results = pd.concat([df_chunking_results, new_row], ignore_index = True)\n","    else:\n","        print(f\"Chunk size {chunk_size} already evaluated. Skipping...\")\n","\n","# Printing the evaluation results\n","print(\"Evaluation results:\")\n","print(df_chunking_results)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Setting up the plot\n","fig, ax = plt.subplots(figsize = (12, 6))\n","\n","# Getting the metrics and their display names\n","metrics = ['answer_correctness', 'answer_relevancy', 'faithfulness', 'context_recall', 'context_precision']\n","metric_display_names = ['Answer Correctness', 'Answer Relevancy', 'Faithfulness', 'Context Recall', 'Context Precision']\n","\n","# Setting up the x-axis\n","x = range(len(df_chunking_results['chunk_size']))\n","width = 0.15\n","\n","# Plotting each metric\n","for i, (metric, display_name) in enumerate(zip(metrics, metric_display_names)):\n","    ax.bar([xi + i * width for xi in x], df_chunking_results[metric], width, label = display_name)\n","\n","# Customizing the plot\n","ax.set_ylabel('Score')\n","ax.set_title('Ragas Metrics Performance by Chunk Size')\n","ax.set_xticks([xi + width * 2 for xi in x])\n","ax.set_xticklabels(df_chunking_results['chunk_size'])\n","ax.set_xlabel('Chunk Size')\n","ax.legend(loc = 'upper left', bbox_to_anchor = (1, 1))\n","\n","# Setting y-axis limit to 1.0\n","ax.set_ylim(0, 1.0)\n","\n","# Adjusting layout and displaying the plot\n","plt.tight_layout()\n","plt.show()\n","\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Saving the results to file\n","df_chunking_results.to_csv('../data/chunking_experiment_results.csv', index = False)"]},{"cell_type":"markdown","metadata":{},"source":["# Optimized Indexing for Fast Retrieval\n","Now that we have covered the chunking strategies, let's move into talking about indexing. To be completely transparent, I struggled to emulate this because pretty much all vector databases come with some sort of indexing algorithm built in! Most commonly, you will run into a particular algorithm called **Hierarchical Navigable Small World (HNSW)**. While there are indeed other options for indexing, we won't necessarily cover those. Again, this is because whether you choose to use Pinecone, Weaviate, AWS OpenSearch, Chroma, FAISS, or one of the other multitude of options out there, you're going to get this indexing optimization built in.\n","\n","Instead, what we'll focus on is comparing retrieval speeds between an index with no special optimizations applied compared to the FAISS in-memory database option that we used in the previous chunking section."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading the documents\n","documents = DataFrameLoader(df_kis, page_content_column = 'ki_text').load()\n","\n","# Creating FAISS index without chunking\n","faiss_index = FAISS.from_documents(documents, embedding_algorithm)\n","\n","# Printing the number of items in FAISS index\n","print(f\"Number of items in FAISS index: {faiss_index.index.ntotal}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creating a standard Python list to store embeddings\n","standard_list_embeddings = []\n","\n","# Embedding documents and storing in the list\n","for doc in documents:\n","    embedding = embedding_algorithm.embed_query(doc.page_content)\n","    standard_list_embeddings.append(embedding)\n","\n","# Printing the length of the list to confirm\n","print(f\"Number of embeddings in standard list: {len(standard_list_embeddings)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def find_most_similar(query_embedding, embeddings_list):\n","    similarities = [np.dot(query_embedding, doc_embedding) for doc_embedding in embeddings_list]\n","    return np.argmax(similarities)\n","\n","faiss_times = []\n","standard_times = []\n","\n","for question in df_validation['question']:\n","    # FAISS retrieval\n","    start_time = time.time()\n","    faiss_index.similarity_search(question, k = 1)\n","    faiss_times.append(time.time() - start_time)\n","    \n","    # Standard list retrieval\n","    start_time = time.time()\n","    query_embedding = embedding_algorithm.embed_query(question)\n","    find_most_similar(query_embedding, standard_list_embeddings)\n","    standard_times.append(time.time() - start_time)\n","\n","print(f\"Average FAISS retrieval time: {sum(faiss_times) / len(faiss_times):.6f} seconds\")\n","print(f\"Average standard list retrieval time: {sum(standard_times) / len(standard_times):.6f} seconds\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Creating data for the plot\n","methods = ['FAISS', 'Standard List']\n","questions = range(1, 11)  # Assuming 10 questions\n","\n","# Creating the line plot\n","plt.figure(figsize = (10, 6))\n","plt.plot(questions, faiss_times[:10], 'b-', label = 'FAISS')\n","plt.plot(questions, standard_times[:10], 'r-', label = 'Standard List')\n","\n","# Adding labels and title\n","plt.xlabel('Question Number')\n","plt.ylabel('Retrieval Time (seconds)')\n","plt.title('Comparison of Retrieval Times: FAISS vs Standard List')\n","plt.legend()\n","\n","# Setting x-axis ticks to whole numbers\n","plt.xticks(questions)\n","\n","# Displaying the plot\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5104322,"sourceId":8796442,"sourceType":"datasetVersion"},{"datasetId":5561282,"sourceId":9198587,"sourceType":"datasetVersion"},{"sourceId":186142459,"sourceType":"kernelVersion"},{"sourceId":188169152,"sourceType":"kernelVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"}},"nbformat":4,"nbformat_minor":4}
