{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_vocab(data):\n",
    "    \"\"\" Given a list of strings, returns a dictionary of words mapping to their frequency count in the data. \"\"\"\n",
    "    vocab = defaultdict(int)\n",
    "    for line in data:\n",
    "        for word in line.split():\n",
    "            # We add ' </w>' to indicate the end of a word\n",
    "            vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\" Given a vocabulary (dictionary mapping words to frequency counts), returns a dictionary of tuples representing the frequency count of pairs of characters in the vocabulary. \"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    \"\"\" Merges the most frequent pair in the vocabulary. \"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def byte_pair_encoding(data, num_merges):\n",
    "    \"\"\" Performs byte pair encoding on the input data. \"\"\"\n",
    "    vocab = get_vocab(data)\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "    return vocab\n",
    "\n",
    "# Example usage:\n",
    "data = [\"this is a test\", \"this is another test\", \"why is this a test\"]\n",
    "num_merges = 10\n",
    "bpe_vocab = byte_pair_encoding(data, num_merges)\n",
    "\n",
    "# Print the vocabulary with BPE merges\n",
    "print(bpe_vocab)\n",
    "print(get_vocab(data))\n",
    "print(get_stats(get_vocab(data)))\n",
    "\n",
    "pairs = get_stats(get_vocab(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = ('e', 's')\n",
    "bigram = re.escape(' '.join(pair))  # This becomes 'e s'\n",
    "p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pairs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"roses are red violets are blue\"\n",
    "\n",
    "# The compiled pattern 'p' will be used to find matches\n",
    "matches = p.finditer(sentence)\n",
    "\n",
    "for match in matches:\n",
    "    print(match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(pairs, key=pairs.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a function `byte_pair_encoding` that returns the vocabulary\n",
    "# and a function `validate_model` that returns a performance metric\n",
    "\n",
    "num_merges_options = [1000, 5000, 10000, 15000, 20000]  # Example merge options\n",
    "vocab_sizes = []\n",
    "validation_scores = []\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def validate_model(model, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Validates a given model's performance on a validation dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained machine learning model to be validated.\n",
    "    - X_val: The input features of the validation dataset.\n",
    "    - y_val: The true labels of the validation dataset.\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: The accuracy of the model on the validation dataset.\n",
    "    \"\"\"\n",
    "    # Make predictions using the model on the validation data\n",
    "    predictions = model.predict(X_val)\n",
    "    \n",
    "    # Calculate the accuracy of the predictions\n",
    "    accuracy = accuracy_score(y_val, predictions)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "for num_merges in num_merges_options:\n",
    "    vocab = byte_pair_encoding(data, num_merges)\n",
    "    vocab_sizes.append(len(vocab))\n",
    "    \n",
    "    # Train your model here with the vocab and evaluate on the validation set\n",
    "    score = validate_model(model, validation_data)\n",
    "    validation_scores.append(score)\n",
    "\n",
    "# Now plot the trends\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(num_merges_options, vocab_sizes, marker='o')\n",
    "plt.title('Vocabulary Size vs. Number of Merges')\n",
    "plt.xlabel('Number of Merges')\n",
    "plt.ylabel('Vocabulary Size')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(num_merges_options, validation_scores, marker='o', color='r')\n",
    "plt.title('Validation Score vs. Number of Merges')\n",
    "plt.xlabel('Number of Merges')\n",
    "plt.ylabel('Validation Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dummy byte_pair_encoding function for demonstration\n",
    "def byte_pair_encoding(data, num_merges):\n",
    "    # This function is a placeholder and does not perform actual BPE.\n",
    "    # It returns a dummy vocabulary size based on num_merges for demonstration.\n",
    "    return {'vocab_size': 1000 + num_merges}\n",
    "\n",
    "# Function to validate the model's performance\n",
    "def validate_model(model, X_val, y_val):\n",
    "    predictions = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Placeholder for the actual validation data\n",
    "validation_data = (X_val, y_val)\n",
    "\n",
    "num_merges_options = [1000, 5000, 10000, 15000, 20000]\n",
    "vocab_sizes = []\n",
    "validation_scores = []\n",
    "\n",
    "for num_merges in num_merges_options:\n",
    "    vocab = byte_pair_encoding(None, num_merges)  # Data parameter is not used in this dummy function\n",
    "    vocab_sizes.append(vocab['vocab_size'])\n",
    "    \n",
    "    score = validate_model(model, *validation_data)\n",
    "    validation_scores.append(score)\n",
    "\n",
    "# Plotting the trends\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(num_merges_options, vocab_sizes, marker='o')\n",
    "plt.title('Vocabulary Size vs. Number of Merges')\n",
    "plt.xlabel('Number of Merges')\n",
    "plt.ylabel('Vocabulary Size')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(num_merges_options, validation_scores, marker='o', color='r')\n",
    "plt.title('Validation Score vs. Number of Merges')\n",
    "plt.xlabel('Number of Merges')\n",
    "plt.ylabel('Validation Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "encoding = tokenizer(example)\n",
    "print(type(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame with columns 'words' and 'entities'\n",
    "data = {\n",
    "    'words': ['Barack', 'Obama', 'was', 'the', '44th', 'president', 'of', 'the', 'USA'],\n",
    "    'entities': ['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert DataFrame to IOB format\n",
    "def convert_to_iob(df):\n",
    "    iob_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        word, entity = row['words'], row['entities']\n",
    "        iob_data.append((word, entity))\n",
    "    return iob_data\n",
    "\n",
    "iob_formatted_data = convert_to_iob(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iob_formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers from source - only needed for versions <= v4.34\n",
    "# pip install git+https://github.com/huggingface/transformers.git\n",
    "# pip install accelerate\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])\n",
    "# <|system|>\n",
    "# You are a friendly chatbot who always responds in the style of a pirate.</s>\n",
    "# <|user|>\n",
    "# How many helicopters can a human eat in one sitting?</s>\n",
    "# <|assistant|>\n",
    "# Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlx_lm import load, generate\n",
    "\n",
    "# model, tokenizer = load(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "# response = generate(model, tokenizer, prompt=\"hello\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064624411d1a4cce8eb11b1efe7e34eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1707ecef98e440ef98369c95313db4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"  # or \"amazon/MistralLite\" for the optimized version\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
