{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3142749a",
   "metadata": {
    "_cell_guid": "ebb8ffdd-0955-4a08-bfa3-3f0ec9f71691",
    "_uuid": "ec840392-58d7-4a7f-a24b-b1d10d523165",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.009302,
     "end_time": "2024-06-26T20:12:30.295939",
     "exception": false,
     "start_time": "2024-06-26T20:12:30.286637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generating a Synthetic Knowledge Dataset Using Generative AI\n",
    "\n",
    "In this notebook, we will be generating a synthetic dataset that we can specifically use for retrieval augmented generation (RAG) purposes. Specifically, what we will be generating is a corpus of documents representing **knowledge items** that one might find in a typical Fortune 500 companies IT helpdesk context. I will personally be leveraging my Perplexity subscription to generate the data, but you are welcome to use any LLM of your choosing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0865c18",
   "metadata": {
    "papermill": {
     "duration": 0.008461,
     "end_time": "2024-06-26T20:12:30.313654",
     "exception": false,
     "start_time": "2024-06-26T20:12:30.305193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2856c47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:12:30.333671Z",
     "iopub.status.busy": "2024-06-26T20:12:30.333247Z",
     "iopub.status.idle": "2024-06-26T20:15:05.930619Z",
     "shell.execute_reply": "2024-06-26T20:15:05.928960Z"
    },
    "papermill": {
     "duration": 155.610601,
     "end_time": "2024-06-26T20:15:05.933698",
     "exception": false,
     "start_time": "2024-06-26T20:12:30.323097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.2.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.10 (from langchain)\n",
      "  Downloading langchain_core-0.2.10-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.82-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2,>=1 (from langchain)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<3,>=1 (from langchain)\n",
      "  Downloading pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.4/109.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests<3,>=2 (from langchain)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.4.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.10->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<0.3.0,>=0.2.10->langchain)\n",
      "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.4 (from pydantic<3,>=1->langchain)\n",
      "  Downloading pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting typing-extensions>=4.6.1 (from pydantic<3,>=1->langchain)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain)\n",
      "  Downloading certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading langchain-0.2.6-py3-none-any.whl (975 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading langchain_core-0.2.10-py3-none-any.whl (332 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.8/332.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.82-py3-none-any.whl (127 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.7.4-py3-none-any.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.0/409.0 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.4.2-py3-none-any.whl (28 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.4/164.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tenacity, PyYAML, packaging, orjson, numpy, multidict, jsonpointer, idna, greenlet, frozenlist, charset-normalizer, certifi, attrs, async-timeout, annotated-types, yarl, SQLAlchemy, requests, pydantic-core, jsonpatch, aiosignal, pydantic, aiohttp, langsmith, langchain-core, langchain-text-splitters, langchain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "keras-cv 0.8.2 requires keras-core, which is not installed.\n",
      "keras-nlp 0.9.3 requires keras-core, which is not installed.\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\n",
      "beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.6 which is incompatible.\n",
      "google-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\n",
      "google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\n",
      "jupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "osmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\n",
      "tensorstore 0.1.56 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.31 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 async-timeout-4.0.3 attrs-23.2.0 certifi-2024.6.2 charset-normalizer-3.3.2 frozenlist-1.4.1 greenlet-3.0.3 idna-3.7 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.6 langchain-core-0.2.10 langchain-text-splitters-0.2.2 langsmith-0.1.82 multidict-6.0.5 numpy-1.26.4 orjson-3.10.5 packaging-24.1 pydantic-2.7.4 pydantic-core-2.18.4 requests-2.32.3 tenacity-8.4.2 typing-extensions-4.12.2 urllib3-2.2.2 yarl-1.9.4\n",
      "Package langchain installed successfully!\n",
      "Collecting langchain-core\n",
      "  Using cached langchain_core-0.2.10-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain-core)\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core)\n",
      "  Using cached langsmith-0.1.82-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core)\n",
      "  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pydantic<3,>=1 (from langchain-core)\n",
      "  Using cached pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core)\n",
      "  Using cached tenacity-8.4.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core)\n",
      "  Using cached orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "Collecting requests<3,>=2 (from langsmith<0.2.0,>=0.1.75->langchain-core)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain-core)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.4 (from pydantic<3,>=1->langchain-core)\n",
      "  Using cached pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting typing-extensions>=4.6.1 (from pydantic<3,>=1->langchain-core)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core)\n",
      "  Using cached certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached langchain_core-0.2.10-py3-none-any.whl (332 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langsmith-0.1.82-py3-none-any.whl (127 kB)\n",
      "Using cached packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Using cached pydantic-2.7.4-py3-none-any.whl (409 kB)\n",
      "Using cached pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Using cached tenacity-8.4.2-py3-none-any.whl (28 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tenacity, PyYAML, packaging, orjson, jsonpointer, idna, charset-normalizer, certifi, annotated-types, requests, pydantic-core, jsonpatch, pydantic, langsmith, langchain-core\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "keras-cv 0.8.2 requires keras-core, which is not installed.\n",
      "keras-nlp 0.9.3 requires keras-core, which is not installed.\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\n",
      "beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.6 which is incompatible.\n",
      "google-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\n",
      "google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\n",
      "jupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "osmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/annotated_types-0.7.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/packaging-24.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/jsonpointer-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langchain_core-0.2.10.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/pydantic_core already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/idna-3.7.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langsmith-0.1.82.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langsmith already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/pydantic already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/annotated_types already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/orjson-3.10.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/charset_normalizer-3.3.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/tenacity already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/urllib3-2.2.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/_yaml already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/typing_extensions.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langchain_core already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/certifi-2024.6.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/jsonpatch.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/charset_normalizer already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/idna already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/jsonpatch-1.33.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/yaml already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/requests-2.32.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/typing_extensions-4.12.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/pydantic_core-2.18.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/packaging already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/PyYAML-6.0.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/requests already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/pydantic-2.7.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/jsonpointer.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/tenacity-8.4.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/orjson already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/bin already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed PyYAML-6.0.1 annotated-types-0.7.0 certifi-2024.6.2 charset-normalizer-3.3.2 idna-3.7 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.10 langsmith-0.1.82 orjson-3.10.5 packaging-24.1 pydantic-2.7.4 pydantic-core-2.18.4 requests-2.32.3 tenacity-8.4.2 typing-extensions-4.12.2 urllib3-2.2.2\n",
      "Package langchain-core installed successfully!\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.2.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain-community)\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain-community)\n",
      "  Using cached SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Using cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain<0.3.0,>=0.2.6 (from langchain-community)\n",
      "  Using cached langchain-0.2.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.10 (from langchain-community)\n",
      "  Using cached langchain_core-0.2.10-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-community)\n",
      "  Using cached langsmith-0.1.82-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2,>=1 (from langchain-community)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting requests<3,>=2 (from langchain-community)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-community)\n",
      "  Using cached tenacity-8.4.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.6->langchain-community)\n",
      "  Using cached langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pydantic<3,>=1 (from langchain<0.3.0,>=0.2.6->langchain-community)\n",
      "  Using cached pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.10->langchain-community)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<0.3.0,>=0.2.10->langchain-community)\n",
      "  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-community)\n",
      "  Using cached orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain-community)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain-community)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain-community)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain-community)\n",
      "  Using cached certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting typing-extensions>=4.6.0 (from SQLAlchemy<3,>=1.4->langchain-community)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain-community)\n",
      "  Using cached greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain-community)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain-community)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.4 (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain-community)\n",
      "  Using cached pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain_community-0.2.6-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached langchain-0.2.6-py3-none-any.whl (975 kB)\n",
      "Using cached langchain_core-0.2.10-py3-none-any.whl (332 kB)\n",
      "Using cached langsmith-0.1.82-py3-none-any.whl (127 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached tenacity-8.4.2-py3-none-any.whl (28 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Using cached greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Using cached orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "Using cached packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Using cached pydantic-2.7.4-py3-none-any.whl (409 kB)\n",
      "Using cached pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tenacity, PyYAML, packaging, orjson, numpy, mypy-extensions, multidict, jsonpointer, idna, greenlet, frozenlist, charset-normalizer, certifi, attrs, async-timeout, annotated-types, yarl, typing-inspect, SQLAlchemy, requests, pydantic-core, marshmallow, jsonpatch, aiosignal, pydantic, dataclasses-json, aiohttp, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "keras-cv 0.8.2 requires keras-core, which is not installed.\n",
      "keras-nlp 0.9.3 requires keras-core, which is not installed.\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\n",
      "beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.6 which is incompatible.\n",
      "google-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\n",
      "google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\n",
      "jupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "osmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\n",
      "tensorstore 0.1.56 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/annotated_types-0.7.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/packaging-24.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/greenlet-3.0.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langchain_text_splitters already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/jsonpointer-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langchain_core-0.2.10.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/pydantic_core already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/idna-3.7.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/greenlet already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langchain_text_splitters-0.2.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langsmith-0.1.82.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langsmith already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/pydantic already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/aiohttp-3.9.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/multidict-6.0.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/annotated_types already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/attr already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/orjson-3.10.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.31 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 async-timeout-4.0.3 attrs-23.2.0 certifi-2024.6.2 charset-normalizer-3.3.2 dataclasses-json-0.6.7 frozenlist-1.4.1 greenlet-3.0.3 idna-3.7 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.6 langchain-community-0.2.6 langchain-core-0.2.10 langchain-text-splitters-0.2.2 langsmith-0.1.82 marshmallow-3.21.3 multidict-6.0.5 mypy-extensions-1.0.0 numpy-1.26.4 orjson-3.10.5 packaging-24.1 pydantic-2.7.4 pydantic-core-2.18.4 requests-2.32.3 tenacity-8.4.2 typing-extensions-4.12.2 typing-inspect-0.9.0 urllib3-2.2.2 yarl-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Target directory /kaggle/working/attrs-23.2.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/charset_normalizer-3.3.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/tenacity already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/attrs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/urllib3-2.2.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/_yaml already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/async_timeout-4.0.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/typing_extensions.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/sqlalchemy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/yarl already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langchain_core already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/certifi-2024.6.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/jsonpatch.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/charset_normalizer already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/yarl-1.9.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/multidict already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/aiohttp already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/async_timeout already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/idna already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/jsonpatch-1.33.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/frozenlist already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/yaml already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langchain already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/requests-2.32.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/aiosignal already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/typing_extensions-4.12.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/frozenlist-1.4.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/pydantic_core-2.18.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/packaging already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/SQLAlchemy-2.0.31.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/PyYAML-6.0.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/requests already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/pydantic-2.7.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/jsonpointer.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/aiosignal-1.3.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/tenacity-8.4.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langchain-0.2.6.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/orjson already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/bin already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/include already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package langchain-community installed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain_openailangchain-text-splitters (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain_openailangchain-text-splitters\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package langchain_openailangchain-text-splitters failed to install. Error: Command '['/opt/conda/bin/python', '-m', 'pip', 'install', 'langchain_openailangchain-text-splitters', '--target=/kaggle/working/']' returned non-zero exit status 1.\n",
      "Collecting ragas\n",
      "  Downloading ragas-0.1.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting numpy (from ragas)\n",
      "  Downloading numpy-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets (from ragas)\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting tiktoken (from ragas)\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting langchain (from ragas)\n",
      "  Using cached langchain-0.2.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting langchain-core (from ragas)\n",
      "  Using cached langchain_core-0.2.10-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain-community (from ragas)\n",
      "  Using cached langchain_community-0.2.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-openai (from ragas)\n",
      "  Downloading langchain_openai-0.1.10-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting openai>1 (from ragas)\n",
      "  Downloading openai-1.35.5-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pysbd>=0.3.4 (from ragas)\n",
      "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting nest-asyncio (from ragas)\n",
      "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting appdirs (from ragas)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai>1->ragas)\n",
      "  Downloading anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>1->ragas)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>1->ragas)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai>1->ragas)\n",
      "  Using cached pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
      "Collecting sniffio (from openai>1->ragas)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai>1->ragas)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions<5,>=4.7 (from openai>1->ragas)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting filelock (from datasets->ragas)\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->ragas)\n",
      "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->ragas)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->ragas)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets->ragas)\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting requests>=2.32.2 (from datasets->ragas)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets->ragas)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->ragas)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets->ragas)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets->ragas)\n",
      "  Using cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets->ragas)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting packaging (from datasets->ragas)\n",
      "  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pyyaml>=5.1 (from datasets->ragas)\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain->ragas)\n",
      "  Using cached SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain->ragas)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain->ragas)\n",
      "  Using cached langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain->ragas)\n",
      "  Using cached langsmith-0.1.82-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy (from ragas)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain->ragas)\n",
      "  Using cached tenacity-8.4.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core->ragas)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community->ragas)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->ragas)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->ragas)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets->ragas)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->ragas)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->ragas)\n",
      "  Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->ragas)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai>1->ragas)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.5.0->openai>1->ragas)\n",
      "  Downloading exceptiongroup-1.2.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas)\n",
      "  Using cached marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai>1->ragas)\n",
      "  Using cached certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>1->ragas)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core->ragas)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain->ragas)\n",
      "  Using cached orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai>1->ragas)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.4 (from pydantic<3,>=1.9.0->openai>1->ragas)\n",
      "  Using cached pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets->ragas)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets->ragas)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain->ragas)\n",
      "  Using cached greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->datasets->ragas)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets->ragas)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets->ragas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets->ragas)\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading ragas-0.1.9-py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.1/86.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.35.5-py3-none-any.whl (327 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.5/327.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached langchain-0.2.6-py3-none-any.whl (975 kB)\n",
      "Using cached langchain_core-0.2.10-py3-none-any.whl (332 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached langchain_community-0.2.6-py3-none-any.whl (2.2 MB)\n",
      "Downloading langchain_openai-0.1.10-py3-none-any.whl (40 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Using cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Downloading anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Using cached langsmith-0.1.82-py3-none-any.whl (127 kB)\n",
      "Using cached packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic-2.7.4-py3-none-any.whl (409 kB)\n",
      "Using cached pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached tenacity-8.4.2-py3-none-any.whl (28 kB)\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Downloading exceptiongroup-1.2.1-py3-none-any.whl (16 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Using cached greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Using cached orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, appdirs, xxhash, urllib3, tzdata, typing-extensions, tqdm, tenacity, sniffio, six, regex, pyyaml, pysbd, pyarrow-hotfix, packaging, orjson, numpy, nest-asyncio, mypy-extensions, multidict, jsonpointer, idna, h11, greenlet, fsspec, frozenlist, filelock, exceptiongroup, distro, dill, charset-normalizer, certifi, attrs, async-timeout, annotated-types, yarl, typing-inspect, SQLAlchemy, requests, python-dateutil, pydantic-core, pyarrow, multiprocess, marshmallow, jsonpatch, httpcore, anyio, aiosignal, tiktoken, pydantic, pandas, huggingface-hub, httpx, dataclasses-json, aiohttp, openai, langsmith, langchain-core, datasets, langchain-text-splitters, langchain-openai, langchain, langchain-community, ragas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "keras-cv 0.8.2 requires keras-core, which is not installed.\n",
      "keras-nlp 0.9.3 requires keras-core, which is not installed.\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\n",
      "beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.6 which is incompatible.\n",
      "gcsfs 2024.2.0 requires fsspec==2024.2.0, but you have fsspec 2024.5.0 which is incompatible.\n",
      "google-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\n",
      "google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\n",
      "jupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "osmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "s3fs 2024.2.0 requires fsspec==2024.2.0, but you have fsspec 2024.5.0 which is incompatible.\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\n",
      "tensorstore 0.1.56 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
      "textblob 0.18.0.post0 requires nltk>=3.8, but you have nltk 3.2.4 which is incompatible.\n",
      "virtualenv 20.21.0 requires platformdirs<4,>=2.4, but you have platformdirs 4.2.0 which is incompatible.\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/annotated_types-0.7.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/packaging-24.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/dataclasses_json already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/greenlet-3.0.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langchain_text_splitters already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed SQLAlchemy-2.0.31 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.4.0 appdirs-1.4.4 async-timeout-4.0.3 attrs-23.2.0 certifi-2024.6.2 charset-normalizer-3.3.2 dataclasses-json-0.6.7 datasets-2.20.0 dill-0.3.8 distro-1.9.0 exceptiongroup-1.2.1 filelock-3.15.4 frozenlist-1.4.1 fsspec-2024.5.0 greenlet-3.0.3 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 huggingface-hub-0.23.4 idna-3.7 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.6 langchain-community-0.2.6 langchain-core-0.2.10 langchain-openai-0.1.10 langchain-text-splitters-0.2.2 langsmith-0.1.82 marshmallow-3.21.3 multidict-6.0.5 multiprocess-0.70.16 mypy-extensions-1.0.0 nest-asyncio-1.6.0 numpy-1.26.4 openai-1.35.5 orjson-3.10.5 packaging-24.1 pandas-2.2.2 pyarrow-16.1.0 pyarrow-hotfix-0.6 pydantic-2.7.4 pydantic-core-2.18.4 pysbd-0.3.4 python-dateutil-2.9.0.post0 pytz-2024.1 pyyaml-6.0.1 ragas-0.1.9 regex-2024.5.15 requests-2.32.3 six-1.16.0 sniffio-1.3.1 tenacity-8.4.2 tiktoken-0.7.0 tqdm-4.66.4 typing-extensions-4.12.2 typing-inspect-0.9.0 tzdata-2024.1 urllib3-2.2.2 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Target directory /kaggle/working/jsonpointer-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langchain_core-0.2.10.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/pydantic_core already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/mypy_extensions.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/idna-3.7.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/greenlet already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langchain_text_splitters-0.2.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langsmith-0.1.82.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langsmith already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/pydantic already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/aiohttp-3.9.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/multidict-6.0.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/annotated_types already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/marshmallow-3.21.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/attr already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/orjson-3.10.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langchain_community already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/attrs-23.2.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/charset_normalizer-3.3.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/tenacity already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/attrs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/urllib3-2.2.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/_yaml already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/async_timeout-4.0.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/typing_inspect-0.9.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/typing_extensions.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/sqlalchemy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/yarl already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langchain_core already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/certifi-2024.6.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/jsonpatch.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langchain_community-0.2.6.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/charset_normalizer already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/yarl-1.9.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/multidict already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/aiohttp already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/async_timeout already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/idna already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/jsonpatch-1.33.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/frozenlist already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/marshmallow already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/yaml already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langchain already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/dataclasses_json-0.6.7.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/requests-2.32.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/aiosignal already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/typing_extensions-4.12.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/frozenlist-1.4.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/pydantic_core-2.18.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/packaging already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/SQLAlchemy-2.0.31.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/PyYAML-6.0.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/requests already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/mypy_extensions-1.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/pydantic-2.7.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/jsonpointer.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/aiosignal-1.3.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/tenacity-8.4.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/langchain-0.2.6.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/orjson already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/typing_inspect.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/bin already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/include already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package ragas installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Installing the Python modules in Kaggle\n",
    "# Note: This cell only needs run once so long as you are persisting the files in your Kaggle notebook. Comment out when no longer needed.\n",
    "pip_installs = [\n",
    "    'langchain',\n",
    "    'langchain-core',\n",
    "    'langchain-community',\n",
    "    'langchain_openai'\n",
    "    'langchain-text-splitters',\n",
    "    'ragas'\n",
    "]\n",
    "from pip_install import perform_pip_install\n",
    "perform_pip_install(pip_installs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2353407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:15:05.980170Z",
     "iopub.status.busy": "2024-06-26T20:15:05.979717Z",
     "iopub.status.idle": "2024-06-26T20:15:09.247016Z",
     "shell.execute_reply": "2024-06-26T20:15:09.244196Z"
    },
    "papermill": {
     "duration": 3.295302,
     "end_time": "2024-06-26T20:15:09.251417",
     "exception": false,
     "start_time": "2024-06-26T20:15:05.956115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.testset.generator import TestsetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ff27e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:15:09.328585Z",
     "iopub.status.busy": "2024-06-26T20:15:09.327482Z",
     "iopub.status.idle": "2024-06-26T20:15:09.679579Z",
     "shell.execute_reply": "2024-06-26T20:15:09.678237Z"
    },
    "papermill": {
     "duration": 0.395973,
     "end_time": "2024-06-26T20:15:09.682309",
     "exception": false,
     "start_time": "2024-06-26T20:15:09.286336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the API key per the appropriate environment\n",
    "if 'KAGGLE_URL_BASE' in os.environ:\n",
    "    \n",
    "    # Loading the API keys from Kaggle secrets\n",
    "    from load_api_keys import load_api_keys\n",
    "    API_KEYS = load_api_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8e9a048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:15:09.729869Z",
     "iopub.status.busy": "2024-06-26T20:15:09.729416Z",
     "iopub.status.idle": "2024-06-26T20:15:09.763471Z",
     "shell.execute_reply": "2024-06-26T20:15:09.762384Z"
    },
    "papermill": {
     "duration": 0.061495,
     "end_time": "2024-06-26T20:15:09.766695",
     "exception": false,
     "start_time": "2024-06-26T20:15:09.705200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiating the chat model\n",
    "chat_model = ChatOpenAI(api_key = API_KEYS['PERPLEXITY_API_KEY'],\n",
    "                        base_url = 'https://api.perplexity.ai',\n",
    "                        model = 'llama-3-70b-instruct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5436bb5",
   "metadata": {
    "papermill": {
     "duration": 0.021744,
     "end_time": "2024-06-26T20:15:09.811030",
     "exception": false,
     "start_time": "2024-06-26T20:15:09.789286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "In this section, we'll set up the prompts that we will be using to generate our specific synthetic dataset. Specifically, we are going to need two separate prompts: one to generate the list of knowledge item topics and another to generate a body of text for a knowledge item given a specific knowledge item topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "372d59d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:15:09.857718Z",
     "iopub.status.busy": "2024-06-26T20:15:09.857310Z",
     "iopub.status.idle": "2024-06-26T20:15:09.864896Z",
     "shell.execute_reply": "2024-06-26T20:15:09.863718Z"
    },
    "papermill": {
     "duration": 0.034176,
     "end_time": "2024-06-26T20:15:09.867384",
     "exception": false,
     "start_time": "2024-06-26T20:15:09.833208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the topic generation prompt\n",
    "TOPIC_GENERATION_PROMPT = '''You are the head of an IT helpdesk as part of a Fortune 500 company, and your company maintains a knowledge management system with articles used by helpdesk specialists to answer questions related to IT support. It does not matter what kind of Fortune 500 company this is. Each individual knowledge item in the knowledge management system has a title and supporting body of text. An example of a knowledge item title might be \"Resetting a User's Password.\" Please generate a list of 100 example knowledge item titles that might be found in your knowledge management system. Only include the list of the 100 examples. Do not provide any additional commentary. Please do not say something like \"Here are the 100 examples.\"'''\n",
    "\n",
    "# Creating the prompt to generate the knowledge item body of text\n",
    "KNOWLEDGE_ITEM_GENERATION_PROMPT = '''You are the head of an IT helpdesk as part of a Fortune 500 company, and your company maintains a knowledge management system with articles used by helpdesk specialists to answer questions related to IT support. It does not matter what kind of Fortune 500 company this is. Each individual knowledge item in the knowledge management system has a title and supporting body of text. Within triple backticks below is an example of a title of one of these knowledge items. Please write a body of text of steps that might be associated to the knowledge article. The body of text should be no longer than 1000 words. Only return the text that would be populated into the body of the knowledge article; do not return any other text.\n",
    "\n",
    "Knowledge item title:\n",
    "```\n",
    "{ki_title}\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed0f2b1",
   "metadata": {
    "papermill": {
     "duration": 0.022941,
     "end_time": "2024-06-26T20:15:09.912949",
     "exception": false,
     "start_time": "2024-06-26T20:15:09.890008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generating the Knowledge Item Topics\n",
    "Now that we have set our prompt engineering appropriately, we're ready to begin using it to generate the list of knowledge item topics. We will be leaning on LangChain to help with this. Specifically, we are going to chain together the prompt engineering, chat model itself, and a special parser that will take the output from the chat model and turn it into a Python list that we can iterate over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e1133bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:15:09.961417Z",
     "iopub.status.busy": "2024-06-26T20:15:09.960956Z",
     "iopub.status.idle": "2024-06-26T20:15:09.968111Z",
     "shell.execute_reply": "2024-06-26T20:15:09.966777Z"
    },
    "papermill": {
     "duration": 0.034368,
     "end_time": "2024-06-26T20:15:09.971089",
     "exception": false,
     "start_time": "2024-06-26T20:15:09.936721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiating the output parser\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Setting up the chat prompt template\n",
    "ki_topic_generation_prompt_template = PromptTemplate(\n",
    "    template = '{ki_topic_generation_prompt}\\n{format_instructions}',\n",
    "    input_variables = ['ki_topic_generation_prompt'],\n",
    "    partial_variables = {'format_instructions': output_parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b49cd03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:15:10.020076Z",
     "iopub.status.busy": "2024-06-26T20:15:10.019419Z",
     "iopub.status.idle": "2024-06-26T20:15:10.025745Z",
     "shell.execute_reply": "2024-06-26T20:15:10.024310Z"
    },
    "papermill": {
     "duration": 0.035372,
     "end_time": "2024-06-26T20:15:10.028826",
     "exception": false,
     "start_time": "2024-06-26T20:15:09.993454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chaining the KI topic generation prompt template, chat model, and output parser\n",
    "ki_topic_generation_chain = ki_topic_generation_prompt_template | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf1573",
   "metadata": {
    "papermill": {
     "duration": 0.02308,
     "end_time": "2024-06-26T20:15:10.075327",
     "exception": false,
     "start_time": "2024-06-26T20:15:10.052247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(Note: The following cell has been intentionally commented out for officially saving a new version of the notebook to Kaggle. To use, simply uncomment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db66e3e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:15:10.124248Z",
     "iopub.status.busy": "2024-06-26T20:15:10.123845Z",
     "iopub.status.idle": "2024-06-26T20:15:10.130358Z",
     "shell.execute_reply": "2024-06-26T20:15:10.128667Z"
    },
    "papermill": {
     "duration": 0.032867,
     "end_time": "2024-06-26T20:15:10.133146",
     "exception": false,
     "start_time": "2024-06-26T20:15:10.100279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Generating the KI topics list\n",
    "# ki_topics_list = ki_topic_generation_chain.invoke({\n",
    "#     'ki_topic_generation_prompt': TOPIC_GENERATION_PROMPT\n",
    "# })\n",
    "\n",
    "# # Slimming down the list to ensure there are only a specific amount of topics\n",
    "# ki_topics_list = ki_topics_list[-100:]\n",
    "\n",
    "# # Creating a Pandas DataFrame around the topics list\n",
    "# df_knowledge_items = pd.DataFrame(data = {'ki_topic': ki_topics_list})\n",
    "\n",
    "# # Prepping to add the KI text\n",
    "# df_knowledge_items['ki_text'] = ''\n",
    "\n",
    "# # Saving the knowledge item topics\n",
    "# df_knowledge_items.to_csv('synthetic_ki_topics.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f64ea30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T23:06:58.930868Z",
     "iopub.status.busy": "2024-05-18T23:06:58.930233Z",
     "iopub.status.idle": "2024-05-18T23:06:58.945285Z",
     "shell.execute_reply": "2024-05-18T23:06:58.943908Z",
     "shell.execute_reply.started": "2024-05-18T23:06:58.930835Z"
    },
    "papermill": {
     "duration": 0.022952,
     "end_time": "2024-06-26T20:15:10.178320",
     "exception": false,
     "start_time": "2024-06-26T20:15:10.155368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generating the Knowledge Item Text\n",
    "Now that we have generated our list of topics, we are ready to produce text per each knowledge item topics. Now, I'm honestly a bit worried about something erroring out as we get into this, so to ensure that we're not re-doing work that we've already done, we're going to be constantly saving this text back to `df_knowledge_items` so that if we have to re-do anything, we'll simply skip over the stuff already completed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6222d306",
   "metadata": {
    "papermill": {
     "duration": 0.023756,
     "end_time": "2024-06-26T20:15:10.225230",
     "exception": false,
     "start_time": "2024-06-26T20:15:10.201474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(Note: The following cell has been intentionally commented out for officially saving a new version of the notebook to Kaggle. To use, simply uncomment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e44a475a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:15:10.276976Z",
     "iopub.status.busy": "2024-06-26T20:15:10.276493Z",
     "iopub.status.idle": "2024-06-26T20:15:10.282769Z",
     "shell.execute_reply": "2024-06-26T20:15:10.281244Z"
    },
    "papermill": {
     "duration": 0.037257,
     "end_time": "2024-06-26T20:15:10.286274",
     "exception": false,
     "start_time": "2024-06-26T20:15:10.249017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the knowledge item topics back in from the checkpoint file\n",
    "# df_knowledge_items = pd.read_csv('synthetic_ki_topics.csv')\n",
    "# df_knowledge_items['ki_text'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "433e8bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:15:10.337648Z",
     "iopub.status.busy": "2024-06-26T20:15:10.337243Z",
     "iopub.status.idle": "2024-06-26T20:15:10.344144Z",
     "shell.execute_reply": "2024-06-26T20:15:10.342498Z"
    },
    "papermill": {
     "duration": 0.037315,
     "end_time": "2024-06-26T20:15:10.346631",
     "exception": false,
     "start_time": "2024-06-26T20:15:10.309316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the prompt engineering template to generate the knowledge item text\n",
    "ki_text_generation_prompt = ChatPromptTemplate.from_messages(messages = [\n",
    "    HumanMessagePromptTemplate.from_template(template = KNOWLEDGE_ITEM_GENERATION_PROMPT)\n",
    "])\n",
    "\n",
    "# Creating the inference chain to generate the knowledge item text\n",
    "ki_text_chain = ki_text_generation_prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a78c0120",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:15:10.395022Z",
     "iopub.status.busy": "2024-06-26T20:15:10.394565Z",
     "iopub.status.idle": "2024-06-26T20:15:10.402042Z",
     "shell.execute_reply": "2024-06-26T20:15:10.400684Z"
    },
    "papermill": {
     "duration": 0.03427,
     "end_time": "2024-06-26T20:15:10.404791",
     "exception": false,
     "start_time": "2024-06-26T20:15:10.370521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_ki_text(row):\n",
    "    '''\n",
    "    Generates simulated knowledge item text per a given knowledge item topic\n",
    "    \n",
    "    Inputs:\n",
    "        - row (Pandas DataFrame record): A single record from the Pandas DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        - ki_text (str): The knowledge item text generated by the AI model per the record\n",
    "    '''\n",
    "    \n",
    "    # Checking to see if the knowledge item text has already been generated\n",
    "    if row['ki_text'] == '':\n",
    "        \n",
    "        # Generating the knowledge item text\n",
    "        ki_text = ki_text_chain.invoke({'ki_title': row['ki_topic']}).content\n",
    "        \n",
    "        return ki_text\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Returning what is already in place if the string is not empty\n",
    "        return row['ki_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc6a39f",
   "metadata": {
    "papermill": {
     "duration": 0.023385,
     "end_time": "2024-06-26T20:15:10.451946",
     "exception": false,
     "start_time": "2024-06-26T20:15:10.428561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(Note: The following cell has been intentionally commented out for officially saving a new version of the notebook to Kaggle. To use, simply uncomment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0001d37b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:15:10.500275Z",
     "iopub.status.busy": "2024-06-26T20:15:10.499005Z",
     "iopub.status.idle": "2024-06-26T20:15:10.505003Z",
     "shell.execute_reply": "2024-06-26T20:15:10.503472Z"
    },
    "papermill": {
     "duration": 0.033146,
     "end_time": "2024-06-26T20:15:10.507824",
     "exception": false,
     "start_time": "2024-06-26T20:15:10.474678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Generating the knowledge item text for any topic that hasn't already been accounted for\n",
    "# df_knowledge_items['ki_text'] = df_knowledge_items.apply(generate_ki_text, axis = 1)\n",
    "\n",
    "# # Saving out the final dataset\n",
    "# df_knowledge_items.to_csv('synthetic_knowledge_items.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a20357",
   "metadata": {
    "papermill": {
     "duration": 0.022277,
     "end_time": "2024-06-26T20:15:10.552132",
     "exception": false,
     "start_time": "2024-06-26T20:15:10.529855",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Alternative Knowledge Item Text\n",
    "To demonstrate the potential complications around comingling similar knowledge items in the same index, we will be adding a new set of alternative knowledge item text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e902800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:15:10.664583Z",
     "iopub.status.busy": "2024-06-26T20:15:10.664173Z",
     "iopub.status.idle": "2024-06-26T20:15:10.669285Z",
     "shell.execute_reply": "2024-06-26T20:15:10.668168Z"
    },
    "papermill": {
     "duration": 0.030844,
     "end_time": "2024-06-26T20:15:10.671796",
     "exception": false,
     "start_time": "2024-06-26T20:15:10.640952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the original data from file\n",
    "# df_knowledge_items = pd.read_csv('/kaggle/input/synthetic-it-related-knowledge-items/synthetic_knowledge_items.csv')\n",
    "# df_knowledge_items['alt_ki_text'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff218045",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:15:10.717279Z",
     "iopub.status.busy": "2024-06-26T20:15:10.716901Z",
     "iopub.status.idle": "2024-06-26T20:15:10.723000Z",
     "shell.execute_reply": "2024-06-26T20:15:10.721868Z"
    },
    "papermill": {
     "duration": 0.031757,
     "end_time": "2024-06-26T20:15:10.725403",
     "exception": false,
     "start_time": "2024-06-26T20:15:10.693646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the prompt to generate the alternative knowledge item body of text\n",
    "ALT_KNOWLEDGE_ITEM_GENERATION_PROMPT = '''You are the head of an IT helpdesk as part of a fake Fortune 500 company called \"Widgetco\", and your company maintains a knowledge management system with articles used by helpdesk specialists to answer questions related to IT support. It does not matter what kind of company this is. Each individual knowledge item in the knowledge management system has a title and supporting body of text. Within triple backticks below is an example of a title of one of these knowledge items. Please write a body of text of steps that might be associated to the knowledge article. The body of text should be no longer than 1000 words. Only return the text that would be populated into the body of the knowledge article; do not return any other text. Please also ensure the company name, \"Widgetco\", is referenced at least once in the body of the knowledge article text.\n",
    "\n",
    "Knowledge item title:\n",
    "```\n",
    "{ki_title}\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f299be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:15:10.771675Z",
     "iopub.status.busy": "2024-06-26T20:15:10.771270Z",
     "iopub.status.idle": "2024-06-26T20:15:10.777780Z",
     "shell.execute_reply": "2024-06-26T20:15:10.776634Z"
    },
    "papermill": {
     "duration": 0.032801,
     "end_time": "2024-06-26T20:15:10.780361",
     "exception": false,
     "start_time": "2024-06-26T20:15:10.747560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the prompt engineering template to generate the knowledge item text\n",
    "alt_ki_text_generation_prompt = ChatPromptTemplate.from_messages(messages = [\n",
    "    HumanMessagePromptTemplate.from_template(template = ALT_KNOWLEDGE_ITEM_GENERATION_PROMPT)\n",
    "])\n",
    "\n",
    "# Creating the inference chain to generate the knowledge item text\n",
    "alt_ki_text_chain = alt_ki_text_generation_prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e95de95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:15:10.827126Z",
     "iopub.status.busy": "2024-06-26T20:15:10.826180Z",
     "iopub.status.idle": "2024-06-26T20:15:10.833034Z",
     "shell.execute_reply": "2024-06-26T20:15:10.831955Z"
    },
    "papermill": {
     "duration": 0.032858,
     "end_time": "2024-06-26T20:15:10.835685",
     "exception": false,
     "start_time": "2024-06-26T20:15:10.802827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_alt_ki_text(row):\n",
    "    '''\n",
    "    Generates simulated alternative knowledge item text per a given knowledge item topic\n",
    "    \n",
    "    Inputs:\n",
    "        - row (Pandas DataFrame record): A single record from the Pandas DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        - alt_ki_text (str): The alternative knowledge item text generated by the AI model per the record\n",
    "    '''\n",
    "    \n",
    "    # Checking to see if the knowledge item text has already been generated\n",
    "    if row['alt_ki_text'] == '':\n",
    "        \n",
    "        # Generating the knowledge item text\n",
    "        alt_ki_text = alt_ki_text_chain.invoke({'ki_title': row['ki_topic']}).content\n",
    "        \n",
    "        return alt_ki_text\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Returning what is already in place if the string is not empty\n",
    "        return row['alt_ki_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f623a5f",
   "metadata": {
    "papermill": {
     "duration": 0.02187,
     "end_time": "2024-06-26T20:15:10.879559",
     "exception": false,
     "start_time": "2024-06-26T20:15:10.857689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(Note: The following cell has been intentionally commented out for officially saving a new version of the notebook to Kaggle. To use, simply uncomment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b90aaa5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T20:15:10.925600Z",
     "iopub.status.busy": "2024-06-26T20:15:10.925183Z",
     "iopub.status.idle": "2024-06-26T20:15:10.930429Z",
     "shell.execute_reply": "2024-06-26T20:15:10.929224Z"
    },
    "papermill": {
     "duration": 0.031661,
     "end_time": "2024-06-26T20:15:10.933133",
     "exception": false,
     "start_time": "2024-06-26T20:15:10.901472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Generating the alt knowledge item text for any topic that hasn't already been accounted for\n",
    "# df_knowledge_items['alt_ki_text'] = df_knowledge_items.apply(generate_alt_ki_text, axis = 1)\n",
    "\n",
    "# # Saving out the final dataset\n",
    "# df_knowledge_items.to_csv('synthetic_knowledge_items.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5104322,
     "sourceId": 8543535,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 178377740,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 180669035,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 165.00069,
   "end_time": "2024-06-26T20:15:11.879444",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-26T20:12:26.878754",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
