{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Advanced Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/vscode/.local/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAIBase has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/vscode/.local/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAICommon has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/vscode/.local/lib/python3.11/site-packages/ragas/metrics/__init__.py:4: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._answer_correctness import AnswerCorrectness, answer_correctness\n",
      "/home/vscode/.local/lib/python3.11/site-packages/ragas/metrics/__init__.py:7: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._context_entities_recall import (\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary Python libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the data from the CSV files\n",
    "df_kis = pd.read_csv('../data/synthetic_knowledge_items.csv')\n",
    "df_validation = pd.read_csv('../data/rag_sample_qas_from_kis.csv')\n",
    "\n",
    "# Dropping alt_ki_text from the df_kis DataFrame\n",
    "df_kis.drop(columns = ['alt_ki_text'], inplace = True)\n",
    "\n",
    "# Dropping any unnecessary columns from the validation DataFrame\n",
    "df_validation.drop(columns = ['ki_topic', 'ki_text'], inplace = True)\n",
    "\n",
    "# Renaming the remaining columns\n",
    "df_validation.rename(columns = {\n",
    "    'sample_question': 'question',\n",
    "    'sample_ground_truth': 'ground_truth'\n",
    "}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the embedding algorithm\n",
    "embedding_algorithm = OpenAIEmbeddings(model = 'text-embedding-3-large')\n",
    "\n",
    "# Setting the chat model\n",
    "chat_model = ChatOpenAI(model = 'gpt-4o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database Setup\n",
    "Since we already covered advanced ingestion techniques in a different notebook, we'll quickly set ourselves up here with a vector database that we can use for practicing our advanced retrieval techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the filepath for the index file\n",
    "index_file = '../data/semantic_index.bin'\n",
    "\n",
    "# Checking if the index file exists\n",
    "if os.path.exists(index_file):\n",
    "\n",
    "    # Load the index from file\n",
    "    faiss_index = FAISS.load_local(index_file,\n",
    "                                   embeddings = OpenAIEmbeddings(),\n",
    "                                   allow_dangerous_deserialization = True)\n",
    "\n",
    "# Creating the FAISS index from scratch\n",
    "else:\n",
    "\n",
    "    # Loading the documents\n",
    "    documents = DataFrameLoader(df_kis, page_content_column = 'ki_text').load()\n",
    "\n",
    "    # Creating a semantic text splitter\n",
    "    text_splitter = SemanticChunker(embeddings = embedding_algorithm)\n",
    "\n",
    "    # Splitting the documents into chunks\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Creating FAISS index for the current chunk size\n",
    "    faiss_index = FAISS.from_documents(chunks, embedding_algorithm)\n",
    "\n",
    "    # Save the index to file\n",
    "    faiss_index.save_local(index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the FAISS index: 269\n"
     ]
    }
   ],
   "source": [
    "# Getting the number of documents in the FAISS index\n",
    "num_documents = faiss_index.index.ntotal\n",
    "\n",
    "# Printing the number of documents\n",
    "print(f'Number of documents in the FAISS index: {num_documents}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Retrieval Techniques\n",
    "Let's move into talking about the advanced retrieval techniques we'll use! For your own use case, you may want to use a different combination of these various techniques. Keep in mind: some of these techniques WILL add latency your pipeline. In addition to applying these techniques to get better results, you will want to ensure that you are balancing the trade-offs between speed and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE (Hypothetical Document Embeddings)\n",
    "Don't let the name fool you: this is actually a relatively simple technique. The idea is to take a query and generate a hypothetical document embedding for it. This hypothetical document embedding is then used to retrieve documents from the vector database. This is a great technique to use when you have a query that is very different from the documents in your vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a prompt to help us produce the HyDE generation\n",
    "HyDE_PROMPT = '''Generate a brief, factual paragraph that answers the following question: {query}'''\n",
    "\n",
    "# Setting up the model to use for HyDE generation\n",
    "hyde_model = ChatOpenAI(model = 'gpt-4o-mini')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
