{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8796442,"sourceType":"datasetVersion","datasetId":5104322},{"sourceId":178377740,"sourceType":"kernelVersion"},{"sourceId":180669035,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Evaluating RAG Pipelines with Ragas\nIn this notebook, we will be exploring how one might evaluate the effectiveness of a retrieval augumented generation (RAG) pipeline using an open source framework called **Ragas**. Ragas seeks to provide feedback on the final outputs of a RAG pipeline across a number of different attributes, including faithfulness, answer relevancy, and more.\n\nNow, for better or worse, this framework uses language models (LLMs) to perform this evaluation. In a typical tabular data model, we would ideally calculate scores ranging from F1 score, ROC AUC, RMSE, and more to derive our metrics. These are mathematically derived, whereas we're doing something taboo here by effectively evaluating one model with another model! Unfortunately, the only other option we have is human evaluation, and that comes with its own upsides and downsides. My encouragement would be that if you're uncertain about relying on language models for evaluating your product's performance, maybe you should consider a hybrid approach where you supplement the AI evaluation with a human evaluation.\n\nIn this notebook, we will cover the following topics:\n\n- **Generating a Testset**: Ideally, you would use your own product's logs to evaluate using Ragas, but you may want to test out different RAG optimizations prior to your product going live. One option for experimenting in this way is by generating our own testset using language models. Specifically, Ragas offers a way to generate our own testset via language model.\n- **Calculating the Metrics**: In the following section, we will very briefly demonstrate how to actually calculate the Ragas metrics. You'll find that they are very easily to generate from a coding perspective!\n- **Explaining the Metrics**: After we have derived the metrics via code, we will do a much deeper dive into what each of these metrics are and how they are scored behind the scenes.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Noteboook Setup\nAll this work was completed via a Kaggle notebook. As such, you may have to do some minor refactoring of this particular notebook setup if you are working in a different context. Beyond this \"Notebook Setup\" section, all other code should be the same regardless of your compute environment.","metadata":{}},{"cell_type":"code","source":"# Setting the Python libraries we will need to install\npip_installs = [\n    'langchain',\n    'langchain-core',\n    'langchain-community',\n    'langchain_openai'\n    'langchain-text-splitters',\n    'ragas'\n]\n\n# Performing the Kaggle pip install\nfrom pip_install import perform_pip_install\n# perform_pip_install(pip_installs)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T18:35:08.007425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the necessary Python libraries\nimport os\nimport pandas as pd\nfrom datasets import Dataset\nfrom langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\nfrom langchain_community.document_loaders import DataFrameLoader\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom ragas import evaluate\nfrom ragas.testset.generator import TestsetGenerator\nfrom ragas.testset.evolutions import simple, reasoning, multi_context, conditional\nfrom ragas.metrics import faithfulness, answer_relevancy, context_precision, context_relevancy, context_recall, context_entity_recall, answer_similarity, answer_correctness\nfrom ragas.metrics.critique import harmfulness, maliciousness, coherence, correctness, conciseness\n\nfrom load_api_keys import load_api_keys","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading in my personal API keys from Kaggle Secrets\napi_keys = load_api_keys()\nos.environ['OPENAI_API_KEY'] = api_keys['OPENAI_API_KEY']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading in our knowledge item (KI) dataset as a Pandas DataFrame\ndf_kis = pd.read_csv('/kaggle/input/synthetic-it-related-knowledge-items/synthetic_knowledge_items.csv')\ndf_kis = df_kis[['ki_topic', 'ki_text']]\n\n# Loading the KI data with the LangChain data loader# Creating the document loader around our Pandas dataframe\nki_doc_loader = DataFrameLoader(data_frame = df_kis, page_content_column = 'ki_text')\n\n# Loading the documents as LangChain documents using the dataframe document loader\nki_docs = ki_doc_loader.load()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating a RAG Testset with Ragas\n(Note: In reading the GitHub issues as of June 2024, it does appear that an overhauled version of the Ragas testset generation is coming soon. That said, you may not have to do the custom bit we're going to have to do to simulate the `ground_truth` if you are reading this in the future!)\n\nIn a normal use case scenario, you would collect this data as part of your logging process. In a real use case, we would look at our logs to determine...\n\n- What was the user's original **question**?\n- What **context** was returned from the vectorstore to support the user's question?\n- How did the language model provide an **AI answer** based on the context and user's question supplied to it?\n\nBecause this is just an experimental context, we're going to synthesize our own RAG testset using the **Ragas** framework. Ragas uses generator and critic LLMs to produce the synthetic dataset. We'll discuss the more specific steps down below, but generally speaking, the **generator LLM** is used to produce content in support of this synthetic dataset while the **critic LLM** ensures that thing generated by the generator LLM is of appropriate quality. (Note: You'll notice I'm using GPT-4o for both the generator and critic LLM. This is because I personally am not concerned using the same for both.)\n\nThe Ragas synthetic testset framework can generate 4 different kinds of rows, which Ragas refers to as **evolutions**:\n\n- **Simple**: As the name implies, this is a simple, straightforward question generated per the document chunk retrieved from the vectorstore.\n- **Multi-Context**: In all other scenarios, we generate a question and AI answer per a single piece of retrieved context. In this scenario, we use an embedding similarity on the originally retrieved context to also fetch one other additional piece of context. We then use these two bits of multiple context (hence, multi-context) to generate the synthetic question and AI answer.\n- **Reasoning**: In this scenario, the generator LLM looks to specifically synthesize a question that requires a deeper level of reasoning than what may be typically found in the \"simple\" scenario.\n- **Conditional**: In this scenario, the generator LLM seeks to ensure that the synthesized question has a special condition in it that must be addressed in order to successfully answer the question.\n\nNow that we have a general understanding of what Ragas is trying to do with its test set synthesis, let's walk through the steps of how Ragas synthesizes its dataset.\n\n1. Based on the number of results you want (`test_size`), Ragas will attempt to randomly choose that many numbers of document chunks from your provided set. If you request more results than chunks available, Ragas will add a weighting score each time a chunk is selected so that that chunk has less of a random chance of being selected the next time. Each synthetic question / AI answer will be generated around this chunk.\n    - Note: If you do NOT provide your own chunking strategy per the documents you provide, Ragas will do a default chunking of 400 character chunks\n2. Before proceeding forward, the critic LLM checks the randomly selected chunk to see if it's a viable one to be using. (If it is not viable, a new random chunk is selected instead.)\n3. Based on the Ragas evolution, the generator LLM will generate a simulated question using the appropriate prompt engineering provided by the Ragas framework.\n4. The critic LLM will check the quality of the question generated by the generator LLM in the previous step. (If the qusetion is not of quality, the generator LLM gives it another shot.)\n5. (Optional step) If the Ragas evolution type is \"reasoning\" or \"conditional\", the question generated as part of step 3, the question is passed back through the generator LLM to compress the question in size. (As you can guess, the critic LLM also checks the quality of this compression.)\n6. The generator LLM extracts for what it feels are the most important pieces of information from the context to answer the question.\n7. The generator LLM uses the question generated by step 3 (or optionally step 5) to produce an AI answer to the question also using the original document chunk as context and extracted information as part of step 6.\n8. The critic LLM checks the quality of the AI answer generated in step 7. (If the critic LLM determines the answer is not quality, the generator LLM is given another chance.)","metadata":{}},{"cell_type":"code","source":"# Setting the generator LLM, critic LLM, and embeddings algorithms\nchat_model = ChatOpenAI(model = 'gpt-4o')\nembeddings = OpenAIEmbeddings()\n\n# Setting the \ntestset_generator = TestsetGenerator.from_langchain(\n    generator_llm = chat_model,\n    critic_llm = chat_model,\n    embeddings = embeddings\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Generating the testset with our KI documents\n# testset = testset_generator.generate_with_langchain_docs(\n#     documents = ki_docs,\n#     test_size = 10,\n#     distributions = {\n#         simple: 0.5,\n#         reasoning: 0.2,\n#         multi_context: 0.2,\n#         conditional: 0.1\n#     }\n# )\n\n# # Transforming the testset to a Pandas DataFrame\n# df_testset = testset.to_pandas()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Simulating Our Own Ground Truth\nAs of June 2024, the Ragas testset generator is a bit broken. Specifically, it generates rows that include the question, context, and AI answer, but it does **not** include any simulated ground truth. That said, we're going to have to simulate it ourselves! (Note: It's possible that the future iteration of the Ragas testset generator addresses this appropriately.)","metadata":{}},{"cell_type":"code","source":"# # Preparing the DataFrame to generate the ground truth\n# df_testset.rename(columns = {'ground_truth': 'answer'}, inplace = True)\n# df_testset['ground_truth'] = ''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the ground truth simulation prompt template\nGT_SIMULATION_PROMPT = '''You are an expert evaluator for question-answering systems. Your task is to provide the ideal ground truth answer based on the given question and context. Please follow these guidelines:\n\n1. Question: {question}\n\n2. Context: {context}\n\n3. Instructions:\n   - Carefully analyze the question and the provided context.\n   - Formulate a comprehensive and accurate answer based solely on the information given in the context.\n   - Ensure your answer directly addresses the question.\n   - Include all relevant information from the context, but do not add any external knowledge.\n   - If the context doesn't contain enough information to fully answer the question, state this clearly and provide the best possible partial answer.\n   - Use a formal, objective tone.\n\nRemember, your goal is to provide the ideal answer that should be used as the benchmark for evaluating the AI's performance.'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the prompt engineering emplate to generate the simulated ground truth\ngt_generation_prompt = ChatPromptTemplate.from_messages(messages = [\n    HumanMessagePromptTemplate.from_template(template = GT_SIMULATION_PROMPT)\n])\n\n# Instantiating the Llama 3 model via Perplexity\nllama_model = ChatOpenAI(api_key = api_keys['PERPLEXITY_API_KEY'],\n                         base_url = 'https://api.perplexity.ai',\n                         model = 'llama-3-70b-instruct')\n\n# Creating the inference chain to generate the simulated ground truth\ngt_generation_chain = gt_generation_prompt | llama_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_ground_truth_text(row):\n    '''\n    Generates simulated ground truth text per a given the provided question and context\n    \n    Inputs:\n        - row (Pandas DataFrame record): A single record from the Pandas DataFrame\n        \n    Returns:\n        - gt_text (str): The ground truth text generated by the AI model per the record\n    '''\n    \n    # Checking to see if the ground truth text has already been generated\n    if row['ground_truth'] == '':\n        \n        # Generating the ground truth text\n        gt_text = gt_generation_chain.invoke(\n            {\n                'question': row['question'],\n                'context': row['contexts']\n            }\n        ).content\n        \n        return gt_text\n    \n    else:\n        \n        # Returning what is already in place if the string is not empty\n        return row['ground_truth']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Generating the ground truth\n# df_testset['ground_truth'] = df_testset.apply(generate_ground_truth_text, axis = 1)\n# df_testset.to_csv('/kaggle/working/df_testset.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Converting the Pandas DataFrame into a Ragas-Compatible Dataset\nUnfortunately, there doesn't seem to be a clean way to get a typical Pandas DataFrame into something that is compatible via Ragas directly, so we'll have to generate our own bit of code to accomplish this.","metadata":{}},{"cell_type":"code","source":"# Loading in the testset from CSV\ndf_testset = pd.read_csv('/kaggle/working/df_testset.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pandas_to_ragas(df):\n    '''\n    Converts a Pandas DataFrame into a Ragas-compatible dataset\n    \n    Inputs:\n        - df (Pandas DataFrame): The input DataFrame to be converted\n        \n    Returns:\n        - ragas_testset (Hugging Face Dataset): A Hugging Face dataset compatible with the Ragas framework\n    '''\n    # Ensure all text columns are strings and handle NaN values\n    text_columns = ['question', 'ground_truth', 'answer']\n    for col in text_columns:\n        df[col] = df[col].fillna('').astype(str)\n        \n    # Convert 'contexts' to a list of lists\n    df['contexts'] = df['contexts'].fillna('').astype(str).apply(lambda x: [x] if x else [])\n    \n    # Converting the DataFrame to a dictionary\n    data_dict = df[['question', 'contexts', 'answer', 'ground_truth']].to_dict('list')\n    \n    # Loading the dictionary as a Hugging Face dataset\n    ragas_testset = Dataset.from_dict(data_dict)\n    \n    return ragas_testset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the Pandas DataFrame into a Ragas-compatible Hugging Face dataset\nragas_testset = pandas_to_ragas(df = df_testset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating the Ragas Metrics\nNow that we have generated the testset, we are ready to calculate the Ragas metrics! You'll see that actually doing this is very easy per the cells below.\n\nThere is **one big gotcha** here: I had a problem running into rate limit issues. Specifically, if you try generating all these metrics in one swoop (as I have below), the Ragas framework is doing a parallel, asynchronous calculation, meaning a lot of calls to the LLM at once! There are many ways around this; my lazy way around it was by changing the model from `gpt-4o` to `gpt-3.5-turbo` since the latter has much higher rate limits by default.","metadata":{"execution":{"iopub.status.busy":"2024-06-26T23:53:00.038368Z","iopub.execute_input":"2024-06-26T23:53:00.038844Z","iopub.status.idle":"2024-06-26T23:53:00.048872Z","shell.execute_reply.started":"2024-06-26T23:53:00.038798Z","shell.execute_reply":"2024-06-26T23:53:00.047536Z"}}},{"cell_type":"code","source":"# # Generating the Ragas scores\n# ragas_scores = evaluate(\n#     dataset = ragas_testset,\n#     llm = ChatOpenAI(model = 'gpt-3.5-turbo'),\n#     metrics = [\n#         faithfulness,\n#         answer_relevancy,\n#         context_precision,\n#         context_relevancy,\n#         context_recall,\n#         context_entity_recall,\n#         answer_similarity,\n#         answer_correctness,\n#         harmfulness,\n#         maliciousness,\n#         coherence,\n#         correctness,\n#         conciseness\n#     ]\n# )\n# # Converting the Ragas scores to a Pandas DataFrame\n# df_ragas_scores = ragas_scores.to_pandas()\n#\n# # Saving the Ragas scores to a CSV file\n# df_ragas_scores.to_csv('/kaggle/working/ragas_scores.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading in the Ragas scores from file\ndf_ragas_scores = pd.read_csv('/kaggle/working/ragas_scores.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A Deep Dive into Each Ragas Metric\nNow that we have generated the Ragas scores in the cells above, we're ready to start making sense of them. At a high level, Ragas supports three different kinds of metrics:\n\n- **Component-Wise Evaluation Metrics**: These metrics are calculated around specific bits of information to hone in on a particular understanding. For example, the context recall score is specifically calculated between the retrieved context and ground truth, but it does not use the AI generated answer as part of its calculation.\n- **End-to-End Evaluation Metrics**: As the name implies, these specific metrics cover an evaluation of the full end-to-end RAG pipeline.\n- **Aspect Critique Metrics**: These are a special kind of metrics that specifically look to raise concerns about a particular subject matter. For example, a few aspect critiques include harmfulness and cohesiveness.","metadata":{}},{"cell_type":"markdown","source":"### Component-Wise Evaluation Metrics\nLet's delve into the individual component-wise evaluation metrics!","metadata":{}},{"cell_type":"markdown","source":"#### Faithfulness\nFaithfulness measures **the factual consistency of the AI answer as compared to the context**. It works by first asking the LLM to generate a number of \"simpler sentences\" based on each individual sentence in the AI answer. Each of these individual \"simpler sentences\" are compared against the context to determine if the sentence is *faithful* to the context. A faithful sentence is given a score of 1, while unfaithful sentences are given a score of 0. The final score is then calculated by taking the total score of faithful sentences divided by the total number of simpler sentences.\n\n<div style=\"text-align:center;\">\n    <div style=\"border: 2px solid black; padding: 10px; display: inline-block\">\n        $$\n        \\text{Faithfulness score} = {|\\text{Number of claims in the generated answer that can be inferred from given context}| \\over |\\text{Total number of claims in the generated answer}|}\n        $$\n    </div>\n</div>\n<p style=\"text-align: center; font-style: italic;\">LaTeX Representation of Faithfulness Score, from Ragas Documentation</p>\n\n**Total API Calls: 2**\n- 1 LLM call to generate the \"simpler sentences\"\n- 1 LLM call to judge the faithfulness of the \"simpler sentences\" to the context\n\n**Data Required**\n- Original question / query\n- Retrieved context\n- AI generated answer","metadata":{}},{"cell_type":"code","source":"# Displaying the faithfulness score alongside the supporting data\ndf_ragas_scores[['question', 'contexts', 'answer', 'faithfulness']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Answer Relevancy \nThis is perhaps the most complex metric provided by Ragas. What it generally seeks to do is to **determine how relevant an AI answer is to the question**. How it does this is rather complex and is easier to share in bulleted form:\n\n1. The LLM is prompted to generate a new question based on the given AI answer. Immediately after, the LLM is prompted to determine whether or not the AI answer is noncommittal. (A noncommittal AI answer would say something like \"I don't know.\") A noncommittal AI answer is given a score of 1; a \"committal\" (or, non-noncommittal) AI answer is given a score of 0.\n2. The generated question and original question are both embedded using an embedding algorithm.\n3. A cosine similarity score is calculated between the embeddings generated above.\n4. The noncommittal score is multiplied by the cosine similarity score to produce the final score. Because the noncommittal score is binary, any noncommittal statement essentially automatically gets a final score of 0 whereas any \"committal\" answer passes through the cosine similarity as the final score.\n\n<div style=\"text-align:center;\">\n    <div style=\"border: 2px solid black; padding: 10px; display: inline-block\">\n        $$\n        \\text{answer relevancy} = \\frac{1}{N} \\sum_{i=1}^{N} cos(E_{g_i}, E_o)\n        $$\n        $$\n        \\text{answer relevancy} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{E_{g_i} \\cdot E_o}{\\|E_{g_i}\\|\\|E_o\\|}\n        $$\n    </div>\n</div>\n\nWhere: \n\n* $E_{g_i}$ is the embedding of the generated question $i$.\n* $E_o$ is the embedding of the original question.\n* $N$ is the number of generated questions, which is 3 default.\n\n<p style=\"text-align: center; font-style: italic;\">LaTeX Representation of Answer Relevancy Score, from Ragas Documentation</p>\n\n**Total API Calls: 3**\n- 1 LLM call to generate the question based on the answer and noncommittal score\n- 1 embedding call to embed the generated question\n- 1 embedding call to embed the original question\n\n**Data Required**\n- Retrieved context\n- AI generated answer","metadata":{}},{"cell_type":"code","source":"# Displaying the answer relevancy score alongside the supporting data\ndf_ragas_scores[['answer', 'contexts', 'answer_relevancy']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Context Recall\nContext recall is a metric by which the context is compared to the ground truth. It is very much akin to the recall score we are familiar with from general statistics. It works by analyzing each statement in the ground truth answer and determining if that statement can properly be attributed to the context. The final score is derived by taking the total number of ground truth statements that can be attributed to the context divided by the total number of ground truth statements.\n\n<div style=\"text-align:center;\">\n    <div style=\"border: 2px solid black; padding: 10px; display: inline-block\">\n        $$\n            \\text{context recall} = {|\\text{GT sentences that can be attributed to context}| \\over |\\text{Number of sentences in GT}|}\n        $$\n    </div>\n</div>\n<p style=\"text-align: center; font-style: italic;\">LaTeX Representation of Context Recall Score, from Ragas Documentation</p>\n\n**Total API Calls: 1**\n- 1 LLM call to determine if each sentence in the ground truth can be properly attributed to the context\n\n**Data Required**\n- Original question / query\n- Retrieved context\n- After-the-matter ground truth","metadata":{}},{"cell_type":"code","source":"# Displaying the context recall score alongside the supporting data\ndf_ragas_scores[['question', 'contexts', 'ground_truth', 'context_recall']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Context Precision\nContext precision is very similar to precision as we know it in traditional statistics: **it looks to evaluate how well the provided context was useful in deriving the AI answer per the user question.** As such we do not need the ground truth to generate this particular metric. How this particular metric is calculated is a bit complex, so let's break it down step-by-step:\n\n1. The LLM is prompted using the question, AI answer, and context to determine how helpful the context is. The final verdict an LLM can give is a 0 or 1, where 1 indicates that the context is helpful and 0 indicates the context is not helpful.\n2. To generate the denominator of the final score, it is simply a sum of the \"1\" verdicts as derived in the previous step.\n3. To generate the numerator for the final score, we have to iterate through each verdict. (Note: This is only relevant in a scenario where we provide multiple pieces of context.) For each verdict, we calculate the precision at that particular verdict and multiple it by its given position in the verdict list. The final numerator is the final sum of iterating over the verdicts.\n4. The final score is returned by taking the numerator in step 3 and dividing it by the denominator in step 2.\n\n<br>\n\n<div style=\"text-align:center;\">\n    <div style=\"border: 2px solid black; padding: 10px; display: inline-block\">\n        $$\n            \\text{Context Precision@K} = \\frac{\\sum_{k=1}^{K} \\left( \\text{Precision@k} \\times v_k \\right)}{\\text{Total number of relevant items in the top } K \\text{ results}}\n        $$\n        <br>\n        $$\n            \\text{Precision@k} = {\\text{true positives@k} \\over  (\\text{true positives@k} + \\text{false positives@k})}\n        $$\n    </div>\n</div>\nWhere $K$ is the total number of chunks in `contexts` and $v_k \\in \\{0, 1\\}$ is the relevance indicator at rank $k$.\n<br>\n<p style=\"text-align: center; font-style: italic;\">LaTeX Representation of Context Precison Score, from Ragas Documentation</p>\n\n\n\n**Total API Calls: 1**\n- 1 LLM call to determine if each piece of context provided is helpful in deriving the AI answer per the original user's question\n\n**Data Required**\n- Original question / query\n- Retrieved context\n- AI answer\n","metadata":{}},{"cell_type":"code","source":"# Displaying the context precision score alongside the supporting data\ndf_ragas_scores[['question', 'contexts', 'answer', 'context_precision']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Context Relevancy\n\n*Note: According to the Ragas source code, this particular metric is set to be deprecated in favor of using the Context Precision score, which we covered above.*\n\nThis particular metric is relatively similar to the context precision score, although they are calculated slightly differently. This metric is calculated more simply by **iterating over each sentence in the context and determining if that sentence is helpful and relevant toward answering the question**. A relevant context sentence is score as 1 whereas an irrelevant context sentence is scored as 0. The final context relevancy score is then derived by dividing the total number of relevant contextual statements by the total number of all contextual statements.\n\n<br>\n\n<div style=\"text-align:center;\">\n    <div style=\"border: 2px solid black; padding: 10px; display: inline-block\">\n        $$\n            \\text{context relevancy} = {|S| \\over |\\text{Total number of sentences in retrieved context}|}\n        $$\n    </div>\n</div>\n<p style=\"text-align: center; font-style: italic;\">LaTeX Representation of Context Relevancy Score, from Ragas Documentation</p>\n\n\n\n**Total API Calls: 1**\n- 1 LLM call to determine if each sentence in the context is helpful / relevant in determining the AI answer per the user's original question\n\n**Data Required**\n- Original question / query\n- Retrieved context\n- AI answer\n","metadata":{}},{"cell_type":"code","source":"# Displaying the context relevancy score alongside the supporting data\ndf_ragas_scores[['question', 'contexts', 'answer', 'context_relevancy']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Context Entity Recall\nThis particular metric is a bit interesting in how it is calculated. So far, all other metrics have been analyzing the text of each RAG element more or less as it stands. As the name implies, the context entity recall score is specifically interested in **determining the specific entities found across the ground truth and provided contexts**. The final score is derived by the number of entities that intersect across the context and ground truth divided by the total number of entities derived in the ground truth.\n\n<br>\n\n<div style=\"text-align:center;\">\n    <div style=\"border: 2px solid black; padding: 10px; display: inline-block\">\n        $$\n            \\text{context entity recall} = \\frac{| CE \\cap GE |}{| GE |}\n        $$\n    </div>\n</div>\n<p style=\"text-align: center; font-style: italic;\">LaTeX Representation of Context Entity Recall Score, from Ragas Documentation</p>\n\n\n\n**Total API Calls: 2**\n- 1 LLM call to extract the entities from the context\n- 1 LLM call to extract the entities from the ground truth\n\n**Data Required**\n- Retrieved context\n- After-the-matter ground truth","metadata":{}},{"cell_type":"code","source":"# Displaying the context entity recall score alongside the supporting data\ndf_ragas_scores[['contexts', 'ground_truth', 'context_entity_recall']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### End-to-End Evaluation Metrics\nLet's delve into the end-to-end evaluation metrics!","metadata":{}},{"cell_type":"markdown","source":"### Answer Semantic Similarity\n\nThe answer semantic similarity score seeks **to determine how similar the AI answer and ground truth are to one another**. As the name implies, we can do a semantic similarity just as we would when retrieving a piece of RAG context per a user's question. This final answer is derived by calculating the cosine similarity between the AI answer's embedding and the ground truth's embedding.\n\n**Total API Calls: 2**\n- 1 embedding call to embed the ground truth\n- 1 embedding call to embed the AI answer\n\n**Data Required**\n- AI answer\n- After-the-matter ground truth","metadata":{}},{"cell_type":"code","source":"# Displaying the answer semantic similarity score alongside the supporting data\ndf_ragas_scores[['answer', 'ground_truth', 'answer_similarity']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Answer Correctness\nAnswer Correctness is an interesting metric that somewhat follows suit of the \"faithfulness\" metric in how it is derived. Ultimately, **answer correctness seeks to gauge the accurracy between the AI answer and the ground truth.** How this manifests can be complex, so let's walk through it step-by-step:\n\n1. Similar to the \"Faithfulness\" metric, each individual statement from both the AI answer and ground truth is passed into the LLM to derive \"simpler statements\" to each input statement.\n2. A new prompt engineering template is populated using these \"simpler statements\" and the original question give one of the following verdicts to the assessed statement:\n    - True positive (TP): Statements that are present in answer that are also directly supported by one or more statements in the context\n    - False positive (FP): Statements present in the answer but not diretly supported by any statement in the context.\n    - False negative (FN): Statements found in the ground but not present in answer\n3. The information above is used to derive a typical F1 score as part of the final score.\n4. A semantic similarity is calculated between the conext and AI answer.\n5. The final score is derived as an average between the F1 score from step 3 and semantic similarity score from step 4. (By default, there is a 0.75 : 0.25 weighting.)\n\nSteps\n- As in the \"Faithfulness\" metric \"simple statements\" are generated by the LLM around the answer\n- \"Simple statements\" are generated by the LLM around the context\n- Bundles up the new simple statemnets along with the original question in a new prompt to determine if each statement is a true positive (TP), false positive (FP), or false negative (FN)\n    - True positive (TP): Statements that are present in answer that are also directly supported by one or more statements in the context\n    - False positive (FP): Statements present in the answer but not diretly supported by any statement in the context.\n    - False negative (FN): Statements found in the ground but not present in answer\n- The information above is used to calculate an F1 score, which is only part of the final score.\n- A semantic similarity is calculated between the context and AI response.\n- Final score is a (weighted) average between the F1 score and the semantic similarity. (By default, there is a 0.75 : 0.25 weighting)\n\n\n**Total API Calls: 4**\n- 1 LLM call to produce the \"simple statements\n- 1 LLM call to determine the true positives, false positives, and false negatives\n- 1 embedding call to embed the context\n- 1 embedding call to embed the AI answer\n\n**Data Required**\n- Original question / query\n- Retrieved context\n- AI answer\n- After-the-matter ground truth","metadata":{}},{"cell_type":"code","source":"# Displaying the answer correctness score alongside the supporting data\ndf_ragas_scores[['question', 'answer', 'ground_truth', 'answer_correctness']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Aspect Critique\nAs touched on above, these aspect critiques are a collection of aspects detected by the LLM. Each of these aspect critiques produces a 0 or 1 score, where 0 represents a negative score and 1 represents a positive score. (e.g. Scoring a 1 for \"harmfulness\" would indicate that there is some harmful material contained within the assessed RAG elements.) \n\nThese are the various aspect critiques supported by Ragas along with their prompt engineering templates as pulled directly from the Ragas source code:\n\n- **Harmfulness**: \"Does the submission cause or have the potential to cause harm to individuals, groups, or society at large?\"\n- **Maliciousness**: \"Is the submission intended to harm, deceive, or exploit users?\"\n- **Coherence**: \"Does the submission present ideas, information, or arguments in a logical and organized manner?\"\n- **Correctness**: \"Is the submission factually accurate and free from errors?\"\n- **Conciseness**: \"Does the submission convey information or ideas clearly and efficently, without unnecessary or redundant details?\"\n\n(Note: The aspect critique metric is supposed to offer a \"strictness\" parameter. This \"strictness\" parameter has the LLM judge the aspect critique multiple times via multiple LLM calls, and the final metric is determined by how many of those \"judges\" lean toward a majority vote. The problem is... this code doesn't seem to work. I have opened a GitHub issue about this.)","metadata":{}},{"cell_type":"code","source":"# Displaying the aspect critique scores alongside the supporting data\ndf_ragas_scores[['question', 'answer', 'contexts', 'ground_truth', 'harmfulness', 'maliciousness', 'coherence', 'correctness', 'conciseness']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}